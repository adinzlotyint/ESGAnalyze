{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac9cd781",
   "metadata": {},
   "source": [
    "## **2. Tokenizacja zbiorów danych**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbff71d",
   "metadata": {},
   "source": [
    "### **Uwaga!**\n",
    "Ze względu na poufność danych, surowe raporty i adnotacje ekspertów nie są zawarte w tym repozytorium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba59e66",
   "metadata": {},
   "source": [
    "### **Model bazowy:** speakleash/Bielik-1.5B-v3 (https://huggingface.co/speakleash/Bielik-1.5B-v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb90f0b4",
   "metadata": {},
   "source": [
    "#### **Import Bibliotek**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aea2793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53524a16",
   "metadata": {},
   "source": [
    "Ustawienie sztywnych parametrów dla okna kontekstowego zgodnie z dokumentacją modelu Bielik-1.5B-v3 oraz narzucenie wymiaru okna przesuwnego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fed7bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"speakleash/Bielik-1.5B-v3\"\n",
    "MAX_LENGTH = 8192\n",
    "STRIDE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4221adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_json('data/data_split/train.jsonl', lines=True)\n",
    "df_val = pd.read_json('data/data_split/validation.jsonl', lines=True)\n",
    "df_test = pd.read_json('data/data_split/test.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8a0f49",
   "metadata": {},
   "source": [
    "#### **2.1. Przygotowanie do tokenizacji.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "611dca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "val_dataset = Dataset.from_pandas(df_val)\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3185328",
   "metadata": {},
   "source": [
    "Wczytanie odpowiedniego tokenizera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b80fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108160c9",
   "metadata": {},
   "source": [
    "#### **2.2. Przeprowadzenie tokenizacji.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a7dc5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_and_tokenize(examples):\n",
    "    # Tokenizujemy tekst, co tworzy wiele fragmentów dla długich dokumentów\n",
    "    tokenized_output = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        stride=STRIDE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=False,\n",
    "    )\n",
    "\n",
    "    # Pobieramy mapowanie z nowych, podzielonych fragmentów do oryginalnych przykładów\n",
    "    sample_mapping = tokenized_output.pop(\"overflow_to_sample_mapping\")\n",
    "    \n",
    "    # Tworzymy listę etykiet dla każdego nowego fragmentu\n",
    "    labels = []\n",
    "    for i in range(len(tokenized_output[\"input_ids\"])):\n",
    "        original_sample_idx = sample_mapping[i]\n",
    "        labels.append(examples[\"labels\"][original_sample_idx])\n",
    "    \n",
    "    tokenized_output[\"labels\"] = labels\n",
    "    return tokenized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa8a8e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4ae5ea6928466b84ecdf7a8d34b3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/279 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293c438d8b974f7db2a1b2c68bbdc329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/57 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52670ed31ccf4cf68ab7c3394668d00c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/57 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    chunk_and_tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    "    num_proc=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dcceb1",
   "metadata": {},
   "source": [
    "#### **2.3. Zapis wyników tokenizacji.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ac0cb52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ecb2cb472e420d9a961605bd670085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3475 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b85784b91534e988111f2b73c9e422c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/563 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c67b85bfd9548e8839ef6d2af8a4720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/683 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "OUTPUT_DIR = \"data/data_tokenized\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "tokenized_datasets.save_to_disk(OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
