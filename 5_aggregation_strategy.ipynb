{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69b42e30",
   "metadata": {},
   "source": [
    "## **5. Dob贸r strategii agregacji wynik贸w**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb5648",
   "metadata": {},
   "source": [
    "#### **Import Bibliotek**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d70758dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Callable, List\n",
    "import functools\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from datasets import load_from_disk, Dataset, Sequence, Value\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import (\n",
    "    LongformerForSequenceClassification,\n",
    "    LongformerTokenizerFast,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747be4bc",
   "metadata": {},
   "source": [
    "#### **Parametry stae**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be76084",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZED_DATA_PATH = \"data/data_tokenized\"\n",
    "TRAINED_MODEL_PATH = \"models/run-21b6e434b35f4028a784f1c0711662a1/final\" # Przykadowa cie偶ka\n",
    "MLFLOW_EXPERIMENT_NAME = \"ESGAnalyzeModel-Aggregation-Comparison\"\n",
    "\n",
    "CRITERIA_NAMES = [\n",
    "    'c1_transition_plan', 'c2_risk_management', 'c4_boundaries',\n",
    "    'c6_historical_data', 'c7_intensity_metrics', 'c8_targets_credibility',\n",
    "]\n",
    "NUM_LABELS = len(CRITERIA_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b0dd4",
   "metadata": {},
   "source": [
    "#### **Definicje strategii agregacji**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdda7194",
   "metadata": {},
   "source": [
    "#### Strategia oparta na K-max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7072d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_max_pooling_aggregation(probs: np.ndarray, k: int) -> np.ndarray:\n",
    "    \"\"\"Urednia k-najwy偶szych prawdopodobiestw dla ka偶dej etykiety.\"\"\"\n",
    "    # Sortuj prawdopodobiestwa wzdu偶 osi fragment贸w (axis=0) malejco\n",
    "    sorted_probs = np.sort(probs, axis=0)[::-1]\n",
    "    # Wybierz k-najwy偶szych i oblicz redni\n",
    "    top_k = sorted_probs[:k]\n",
    "    return np.mean(top_k, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8fecd4",
   "metadata": {},
   "source": [
    "#### Strategia oparta na percentylach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7ea491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile_aggregation(probs: np.ndarray, percentile: int) -> np.ndarray:\n",
    "    \"\"\"Oblicza zadany percentyl prawdopodobiestw dla ka偶dej etykiety.\"\"\"\n",
    "    return np.percentile(probs, percentile, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b8c7f0",
   "metadata": {},
   "source": [
    "#### Sownik z parametrami dla strategii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "641481f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGGREGATION_STRATEGIES: Dict[str, Callable[[np.ndarray], np.ndarray]] = {\n",
    "    \"percentile_75\": functools.partial(percentile_aggregation, percentile=75),\n",
    "    \"percentile_85\": functools.partial(percentile_aggregation, percentile=85),\n",
    "    \"percentile_90\": functools.partial(percentile_aggregation, percentile=90),\n",
    "    \"percentile_95\": functools.partial(percentile_aggregation, percentile=95),\n",
    "    \n",
    "    \"k_max_pooling_k2\": functools.partial(k_max_pooling_aggregation, k=2),\n",
    "    \"k_max_pooling_k3\": functools.partial(k_max_pooling_aggregation, k=3),\n",
    "    \"k_max_pooling_k4\": functools.partial(k_max_pooling_aggregation, k=4),\n",
    "    \"k_max_pooling_k5\": functools.partial(k_max_pooling_aggregation, k=5),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be6dd8b",
   "metadata": {},
   "source": [
    "#### **Funkcje pomocnicze**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5cda9a",
   "metadata": {},
   "source": [
    "#### Konfiguracja i ustawienie eksperymentu MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54af7e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_mlflow():\n",
    "    mlflow.set_tracking_uri(\"databricks\")\n",
    "    w = WorkspaceClient()\n",
    "    user_email = w.current_user.me().user_name\n",
    "    experiment_path = f\"/Users/{user_email}/{MLFLOW_EXPERIMENT_NAME}\"\n",
    "    mlflow.set_experiment(experiment_path)\n",
    "    print(f\"Pomylnie ustawiono eksperyment MLflow: {experiment_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe13669",
   "metadata": {},
   "source": [
    "#### Oblicza metryki F1-score na poziomie dokument贸w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf2e9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_document_metrics(doc_labels: np.ndarray, doc_preds: np.ndarray) -> Dict[str, float]:\n",
    "    results = {\n",
    "        'doc_f1_macro': f1_score(doc_labels, doc_preds, average='macro', zero_division=0),\n",
    "        'num_documents': len(doc_labels)\n",
    "    }\n",
    "    f1_per_label = f1_score(doc_labels, doc_preds, average=None, zero_division=0)\n",
    "    for i, f1 in enumerate(f1_per_label):\n",
    "        results[f'doc_f1_{CRITERIA_NAMES[i]}'] = f1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68b8af4",
   "metadata": {},
   "source": [
    "#### **G贸wna funkcja orkiestrujca proces ewaluacji**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da45e9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pomylnie ustawiono eksperyment MLflow: /Users/adinzlotyint@gmail.com/ESGAnalyzeModel-Aggregation-Comparison\n",
      "Wczytywanie ztokenizowanych danych...\n",
      "Konwertowanie typu danych etykiet na float32...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbd298f03654635a9cdf3a8f03432db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1062 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7788f8299ccd40539db6f6e40f81a259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/929 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wczytywanie wytrenowanego modelu z: models/run-21b6e434b35f4028a784f1c0711662a1/final...\n",
      "Wczytano zoptymalizowane progi klasyfikacyjne.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_110736/2419543102.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wykonywanie predykcji na zbiorze walidacyjnym...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predykcje na poziomie fragment贸w dla zbioru walidacyjnego zostay wygenerowane.\n",
      "--- Testowanie strategii na zbiorze walidacyjnym: percentile_75 ---\n",
      "Wyniki dla 'percentile_75': F1 Macro (dokument): 0.7902\n",
      "!!! Nowa najlepsza strategia: percentile_75 z F1 Macro: 0.7902 !!!\n",
      " View run eval_percentile_75 at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160/runs/deca2941822b45c08fec55ebdbd26212\n",
      "И View experiment at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160\n",
      "--- Testowanie strategii na zbiorze walidacyjnym: percentile_85 ---\n",
      "Wyniki dla 'percentile_85': F1 Macro (dokument): 0.7830\n",
      " View run eval_percentile_85 at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160/runs/669ddc26de23428ab7d2251d99babf2c\n",
      "И View experiment at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160\n",
      "--- Testowanie strategii na zbiorze walidacyjnym: percentile_90 ---\n",
      "Wyniki dla 'percentile_90': F1 Macro (dokument): 0.7765\n",
      " View run eval_percentile_90 at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160/runs/153e9c328c564414942c9dd6bf0f3a17\n",
      "И View experiment at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160\n",
      "--- Testowanie strategii na zbiorze walidacyjnym: percentile_95 ---\n",
      "Wyniki dla 'percentile_95': F1 Macro (dokument): 0.7705\n",
      " View run eval_percentile_95 at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160/runs/1b29370a1de441b3a0b2dedda832b277\n",
      "И View experiment at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160\n",
      "--- Testowanie strategii na zbiorze walidacyjnym: k_max_pooling_k2 ---\n",
      "Wyniki dla 'k_max_pooling_k2': F1 Macro (dokument): 0.7732\n",
      " View run eval_k_max_pooling_k2 at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160/runs/2c356879b528482a939f5579856ce4eb\n",
      "И View experiment at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160\n",
      "--- Testowanie strategii na zbiorze walidacyjnym: k_max_pooling_k3 ---\n",
      "Wyniki dla 'k_max_pooling_k3': F1 Macro (dokument): 0.7798\n",
      " View run eval_k_max_pooling_k3 at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160/runs/03695285c81945cfbea652ff52eb356a\n",
      "И View experiment at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160\n",
      "--- Testowanie strategii na zbiorze walidacyjnym: k_max_pooling_k4 ---\n",
      "Wyniki dla 'k_max_pooling_k4': F1 Macro (dokument): 0.7814\n",
      " View run eval_k_max_pooling_k4 at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160/runs/b9a682010cb44306808fd86b0a7b3d45\n",
      "И View experiment at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160\n",
      "--- Testowanie strategii na zbiorze walidacyjnym: k_max_pooling_k5 ---\n",
      "Wyniki dla 'k_max_pooling_k5': F1 Macro (dokument): 0.7848\n",
      " View run eval_k_max_pooling_k5 at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160/runs/c86ade8ba020474db9f49b8a7ab15014\n",
      "И View experiment at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160\n",
      "\n",
      "--- Zakoczono por贸wnywanie strategii ---\n",
      "Najlepsza znaleziona strategia na zbiorze walidacyjnym: 'percentile_75' (F1 Macro: 0.7902)\n"
     ]
    }
   ],
   "source": [
    "setup_mlflow()\n",
    "\n",
    "# --- 1. Wczytanie danych i wytrenowanego modelu ---\n",
    "print(\"Wczytywanie ztokenizowanych danych...\")\n",
    "tokenized_datasets = load_from_disk(TOKENIZED_DATA_PATH)\n",
    "eval_dataset = tokenized_datasets['validation']\n",
    "test_dataset = tokenized_datasets['test']\n",
    "\n",
    "print(\"Konwertowanie typu danych etykiet na float32...\")\n",
    "new_features = eval_dataset.features.copy()\n",
    "new_features['labels'] = Sequence(feature=Value('float32'))\n",
    "eval_dataset = eval_dataset.cast(new_features)\n",
    "test_dataset = test_dataset.cast(new_features)\n",
    "\n",
    "print(f\"Wczytywanie wytrenowanego modelu z: {TRAINED_MODEL_PATH}...\")\n",
    "model = LongformerForSequenceClassification.from_pretrained(TRAINED_MODEL_PATH)\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained(TRAINED_MODEL_PATH)\n",
    "\n",
    "with open(Path(TRAINED_MODEL_PATH) / \"optimal_thresholds.json\", \"r\") as f:\n",
    "    thresholds_dict = json.load(f)\n",
    "    optimal_thresholds = np.array([thresholds_dict[name] for name in CRITERIA_NAMES])\n",
    "print(\"Wczytano zoptymalizowane progi klasyfikacyjne.\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(output_dir=\"./temp_results\", per_device_eval_batch_size=4, report_to=\"none\"),\n",
    "    data_collator=default_data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# --- 2. Predykcje na zbiorze walidacyjnym ---\n",
    "print(\"Wykonywanie predykcji na zbiorze walidacyjnym...\")\n",
    "preds_output = trainer.predict(eval_dataset)\n",
    "chunk_probs = 1 / (1 + np.exp(-preds_output.predictions))\n",
    "print(\"Predykcje na poziomie fragment贸w dla zbioru walidacyjnego zostay wygenerowane.\")\n",
    "\n",
    "df_eval = pd.DataFrame({\n",
    "    'doc_id': eval_dataset['doc_id'],\n",
    "    'probs': list(chunk_probs),\n",
    "    'labels': list(preds_output.label_ids)\n",
    "})\n",
    "\n",
    "doc_grouped_eval = df_eval.groupby('doc_id')\n",
    "doc_labels_map_eval = doc_grouped_eval['labels'].first().to_dict()\n",
    "\n",
    "# --- 3. Ptla ewaluacyjna na zbiorze walidacyjnym w celu znalezienia najlepszej strategii ---\n",
    "best_strategy_name = None\n",
    "best_f1_macro_score = -1.0\n",
    "\n",
    "for strategy_name, aggregation_func in AGGREGATION_STRATEGIES.items():\n",
    "    print(f\"--- Testowanie strategii na zbiorze walidacyjnym: {strategy_name} ---\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"eval_{strategy_name}\") as run:\n",
    "        mlflow.log_param(\"aggregation_strategy\", strategy_name)\n",
    "        mlflow.log_param(\"base_model_path\", TRAINED_MODEL_PATH)\n",
    "        mlflow.log_params({f\"threshold_{k}\": v for k, v in zip(CRITERIA_NAMES, optimal_thresholds)})\n",
    "\n",
    "        aggregated_probs = doc_grouped_eval['probs'].apply(\n",
    "            lambda x: aggregation_func(np.array(x.tolist()))\n",
    "        )\n",
    "        \n",
    "        doc_probs_np = np.array(aggregated_probs.tolist())\n",
    "        doc_labels_np = np.array(list(doc_labels_map_eval.values()))\n",
    "        doc_preds_np = (doc_probs_np >= optimal_thresholds).astype(int)\n",
    "        \n",
    "        final_metrics = calculate_document_metrics(doc_labels_np, doc_preds_np)\n",
    "        mlflow.log_metrics(final_metrics)\n",
    "        \n",
    "        current_f1 = final_metrics['doc_f1_macro']\n",
    "        print(f\"Wyniki dla '{strategy_name}': F1 Macro (dokument): {current_f1:.4f}\")\n",
    "\n",
    "        # Sprawdzenie i zapisanie najlepszej strategii\n",
    "        if current_f1 > best_f1_macro_score:\n",
    "            best_f1_macro_score = current_f1\n",
    "            best_strategy_name = strategy_name\n",
    "            print(f\"!!! Nowa najlepsza strategia: {best_strategy_name} z F1 Macro: {best_f1_macro_score:.4f} !!!\")\n",
    "\n",
    "print(\"\\n--- Zakoczono por贸wnywanie strategii ---\")\n",
    "print(f\"Najlepsza znaleziona strategia na zbiorze walidacyjnym: '{best_strategy_name}' (F1 Macro: {best_f1_macro_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971f0169",
   "metadata": {},
   "source": [
    "#### **Finalna ewaluacja na zbiorze testowym**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "985fd78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wykonywanie predykcji na zbiorze testowym...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U偶yta strategia: percentile_75\n",
      "  F1 Macro Score (dokument): 0.6963\n",
      "  Wyniki F1-score dla poszczeg贸lnych kryteri贸w:\n",
      "    - c1_transition_plan: 0.8000\n",
      "    - c2_risk_management: 0.6207\n",
      "    - c4_boundaries: 0.6944\n",
      "    - c6_historical_data: 0.7595\n",
      "    - c7_intensity_metrics: 0.6364\n",
      "    - c8_targets_credibility: 0.6667\n",
      "-------------------------------------------------------\n",
      "Wyniki zapisane pomylnie.\n",
      " View run final_test_evaluation at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160/runs/3414674617434849972182960f481d30\n",
      "И View experiment at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160\n",
      "\n",
      "Zakoczono cay proces ewaluacji.\n"
     ]
    }
   ],
   "source": [
    "print(\"Wykonywanie predykcji na zbiorze testowym...\")\n",
    "test_preds_output = trainer.predict(test_dataset)\n",
    "test_chunk_probs = 1 / (1 + np.exp(-test_preds_output.predictions))\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'doc_id': test_dataset['doc_id'],\n",
    "    'probs': list(test_chunk_probs),\n",
    "    'labels': list(test_preds_output.label_ids)\n",
    "})\n",
    "\n",
    "doc_grouped_test = df_test.groupby('doc_id')\n",
    "doc_labels_map_test = doc_grouped_test['labels'].first().to_dict()\n",
    "\n",
    "# Pobranie najlepszej funkcji agregujcej\n",
    "best_aggregation_func = AGGREGATION_STRATEGIES[best_strategy_name]\n",
    "\n",
    "# Agregacja prawdopodobiestw na zbiorze testowym\n",
    "aggregated_test_probs = doc_grouped_test['probs'].apply(\n",
    "    lambda x: best_aggregation_func(np.array(x.tolist()))\n",
    ")\n",
    "\n",
    "doc_probs_test_np = np.array(aggregated_test_probs.tolist())\n",
    "doc_labels_test_np = np.array(list(doc_labels_map_test.values()))\n",
    "\n",
    "# Zastosowanie prog贸w do predykcji\n",
    "doc_preds_test_np = (doc_probs_test_np >= optimal_thresholds).astype(int)\n",
    "\n",
    "# Obliczenie metryk kocowych\n",
    "final_test_metrics = calculate_document_metrics(doc_labels_test_np, doc_preds_test_np)\n",
    "\n",
    "print(f\"U偶yta strategia: {best_strategy_name}\")\n",
    "print(f\"  F1 Macro Score (dokument): {final_test_metrics['doc_f1_macro']:.4f}\")\n",
    "print(\"  Wyniki F1-score dla poszczeg贸lnych kryteri贸w:\")\n",
    "for name in CRITERIA_NAMES:\n",
    "    metric_key = f'doc_f1_{name}'\n",
    "    print(f\"    - {name}: {final_test_metrics.get(metric_key, 0.0):.4f}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "with mlflow.start_run(run_name=\"final_test_evaluation\") as run:\n",
    "    mlflow.log_param(\"best_aggregation_strategy\", best_strategy_name)\n",
    "    mlflow.log_param(\"base_model_path\", TRAINED_MODEL_PATH)\n",
    "    mlflow.log_params({f\"threshold_{k}\": v for k, v in zip(CRITERIA_NAMES, optimal_thresholds)})\n",
    "    \n",
    "    test_metrics_to_log = {f\"test_{k}\": v for k, v in final_test_metrics.items()}\n",
    "    mlflow.log_metrics(test_metrics_to_log)\n",
    "    print(\"Wyniki zapisane pomylnie.\")\n",
    "\n",
    "print(\"\\nZakoczono cay proces ewaluacji.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
