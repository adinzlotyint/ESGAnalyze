{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69b42e30",
   "metadata": {},
   "source": [
    "## **5. Dobór strategii agregacji wyników**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb5648",
   "metadata": {},
   "source": [
    "#### **Import Bibliotek**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d70758dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Callable, List\n",
    "import functools\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from datasets import load_from_disk, Dataset, Sequence, Value\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import (\n",
    "    LongformerForSequenceClassification,\n",
    "    LongformerTokenizerFast,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747be4bc",
   "metadata": {},
   "source": [
    "#### **Parametry stałe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be76084",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZED_DATA_PATH = \"data/data_tokenized\"\n",
    "TRAINED_MODEL_PATH = \"models/run-21b6e434b35f4028a784f1c0711662a1/final\" # Przykładowa ścieżka\n",
    "MLFLOW_EXPERIMENT_NAME = \"ESGAnalyzeModel-Aggregation-Comparison\"\n",
    "\n",
    "CRITERIA_NAMES = [\n",
    "    'c1_transition_plan', 'c2_risk_management', 'c4_boundaries',\n",
    "    'c6_historical_data', 'c7_intensity_metrics', 'c8_targets_credibility',\n",
    "]\n",
    "NUM_LABELS = len(CRITERIA_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b0dd4",
   "metadata": {},
   "source": [
    "#### **Definicje strategii agregacji**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdda7194",
   "metadata": {},
   "source": [
    "#### Strategia oparta na K-max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7072d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_max_pooling_aggregation(probs: np.ndarray, k: int) -> np.ndarray:\n",
    "    \"\"\"Uśrednia k-najwyższych prawdopodobieństw dla każdej etykiety.\"\"\"\n",
    "    # Sortuj prawdopodobieństwa wzdłuż osi fragmentów (axis=0) malejąco\n",
    "    sorted_probs = np.sort(probs, axis=0)[::-1]\n",
    "    # Wybierz k-najwyższych i oblicz średnią\n",
    "    top_k = sorted_probs[:k]\n",
    "    return np.mean(top_k, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8fecd4",
   "metadata": {},
   "source": [
    "#### Strategia oparta na percentylach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7ea491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile_aggregation(probs: np.ndarray, percentile: int) -> np.ndarray:\n",
    "    \"\"\"Oblicza zadany percentyl prawdopodobieństw dla każdej etykiety.\"\"\"\n",
    "    return np.percentile(probs, percentile, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b8c7f0",
   "metadata": {},
   "source": [
    "#### Słownik z parametrami dla strategii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "641481f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGGREGATION_STRATEGIES: Dict[str, Callable[[np.ndarray], np.ndarray]] = {\n",
    "    \"percentile_75\": functools.partial(percentile_aggregation, percentile=75),\n",
    "    \"percentile_85\": functools.partial(percentile_aggregation, percentile=85),\n",
    "    \"percentile_90\": functools.partial(percentile_aggregation, percentile=90),\n",
    "    \"percentile_95\": functools.partial(percentile_aggregation, percentile=95),\n",
    "    \n",
    "    \"k_max_pooling_k2\": functools.partial(k_max_pooling_aggregation, k=2),\n",
    "    \"k_max_pooling_k3\": functools.partial(k_max_pooling_aggregation, k=3),\n",
    "    \"k_max_pooling_k4\": functools.partial(k_max_pooling_aggregation, k=4),\n",
    "    \"k_max_pooling_k5\": functools.partial(k_max_pooling_aggregation, k=5),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be6dd8b",
   "metadata": {},
   "source": [
    "#### **Funkcje pomocnicze**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5cda9a",
   "metadata": {},
   "source": [
    "#### Konfiguracja i ustawienie eksperymentu MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54af7e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_mlflow():\n",
    "    mlflow.set_tracking_uri(\"databricks\")\n",
    "    w = WorkspaceClient()\n",
    "    user_email = w.current_user.me().user_name\n",
    "    experiment_path = f\"/Users/{user_email}/{MLFLOW_EXPERIMENT_NAME}\"\n",
    "    mlflow.set_experiment(experiment_path)\n",
    "    print(f\"Pomyślnie ustawiono eksperyment MLflow: {experiment_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe13669",
   "metadata": {},
   "source": [
    "#### Oblicza metryki F1-score na poziomie dokumentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf2e9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_document_metrics(doc_labels: np.ndarray, doc_preds: np.ndarray) -> Dict[str, float]:\n",
    "    results = {\n",
    "        'doc_f1_macro': f1_score(doc_labels, doc_preds, average='macro', zero_division=0),\n",
    "        'num_documents': len(doc_labels)\n",
    "    }\n",
    "    f1_per_label = f1_score(doc_labels, doc_preds, average=None, zero_division=0)\n",
    "    for i, f1 in enumerate(f1_per_label):\n",
    "        results[f'doc_f1_{CRITERIA_NAMES[i]}'] = f1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68b8af4",
   "metadata": {},
   "source": [
    "#### **Główna funkcja orkiestrująca proces ewaluacji**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da45e9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pomyślnie ustawiono eksperyment MLflow: /Users/adinzlotyint@gmail.com/ESGAnalyzeModel-Aggregation-Comparison\n",
      "Wczytywanie ztokenizowanych danych...\n",
      "Konwertowanie typu danych etykiet na float32...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbd298f03654635a9cdf3a8f03432db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1062 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7788f8299ccd40539db6f6e40f81a259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/929 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wczytywanie wytrenowanego modelu z: models/run-21b6e434b35f4028a784f1c0711662a1/final...\n",
      "Wczytano zoptymalizowane progi klasyfikacyjne.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_110736/2419543102.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wykonywanie predykcji na zbiorze walidacyjnym...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predykcje na poziomie fragmentów dla zbioru walidacyjnego zostały wygenerowane.\n",
      "--- Testowanie strategii na zbiorze walidacyjnym: percentile_75 ---\n",
      "Wyniki dla 'percentile_75': F1 Macro (dokument): 0.7902\n",
      "!!! Nowa najlepsza strategia: percentile_75 z F1 Macro: 0.7902 !!!\n",
      "🏃 View run eval_percentile_75 at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160/runs/deca2941822b45c08fec55ebdbd26212\n",
      "🧪 View experiment at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160\n",
      "--- Testowanie strategii na zbiorze walidacyjnym: percentile_85 ---\n",
      "Wyniki dla 'percentile_85': F1 Macro (dokument): 0.7830\n",
      "🏃 View run eval_percentile_85 at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160/runs/669ddc26de23428ab7d2251d99babf2c\n",
      "🧪 View experiment at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160\n",
      "--- Testowanie strategii na zbiorze walidacyjnym: percentile_90 ---\n",
      "Wyniki dla 'percentile_90': F1 Macro (dokument): 0.7765\n",
      "🏃 View run eval_percentile_90 at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160/runs/153e9c328c564414942c9dd6bf0f3a17\n",
      "🧪 View experiment at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160\n",
      "--- Testowanie strategii na zbiorze walidacyjnym: percentile_95 ---\n",
      "Wyniki dla 'percentile_95': F1 Macro (dokument): 0.7705\n",
      "🏃 View run eval_percentile_95 at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160/runs/1b29370a1de441b3a0b2dedda832b277\n",
      "🧪 View experiment at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160\n",
      "--- Testowanie strategii na zbiorze walidacyjnym: k_max_pooling_k2 ---\n",
      "Wyniki dla 'k_max_pooling_k2': F1 Macro (dokument): 0.7732\n",
      "🏃 View run eval_k_max_pooling_k2 at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160/runs/2c356879b528482a939f5579856ce4eb\n",
      "🧪 View experiment at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160\n",
      "--- Testowanie strategii na zbiorze walidacyjnym: k_max_pooling_k3 ---\n",
      "Wyniki dla 'k_max_pooling_k3': F1 Macro (dokument): 0.7798\n",
      "🏃 View run eval_k_max_pooling_k3 at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160/runs/03695285c81945cfbea652ff52eb356a\n",
      "🧪 View experiment at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160\n",
      "--- Testowanie strategii na zbiorze walidacyjnym: k_max_pooling_k4 ---\n",
      "Wyniki dla 'k_max_pooling_k4': F1 Macro (dokument): 0.7814\n",
      "🏃 View run eval_k_max_pooling_k4 at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160/runs/b9a682010cb44306808fd86b0a7b3d45\n",
      "🧪 View experiment at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160\n",
      "--- Testowanie strategii na zbiorze walidacyjnym: k_max_pooling_k5 ---\n",
      "Wyniki dla 'k_max_pooling_k5': F1 Macro (dokument): 0.7848\n",
      "🏃 View run eval_k_max_pooling_k5 at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160/runs/c86ade8ba020474db9f49b8a7ab15014\n",
      "🧪 View experiment at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160\n",
      "\n",
      "--- Zakończono porównywanie strategii ---\n",
      "Najlepsza znaleziona strategia na zbiorze walidacyjnym: 'percentile_75' (F1 Macro: 0.7902)\n"
     ]
    }
   ],
   "source": [
    "setup_mlflow()\n",
    "\n",
    "# --- 1. Wczytanie danych i wytrenowanego modelu ---\n",
    "print(\"Wczytywanie ztokenizowanych danych...\")\n",
    "tokenized_datasets = load_from_disk(TOKENIZED_DATA_PATH)\n",
    "eval_dataset = tokenized_datasets['validation']\n",
    "test_dataset = tokenized_datasets['test']\n",
    "\n",
    "print(\"Konwertowanie typu danych etykiet na float32...\")\n",
    "new_features = eval_dataset.features.copy()\n",
    "new_features['labels'] = Sequence(feature=Value('float32'))\n",
    "eval_dataset = eval_dataset.cast(new_features)\n",
    "test_dataset = test_dataset.cast(new_features)\n",
    "\n",
    "print(f\"Wczytywanie wytrenowanego modelu z: {TRAINED_MODEL_PATH}...\")\n",
    "model = LongformerForSequenceClassification.from_pretrained(TRAINED_MODEL_PATH)\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained(TRAINED_MODEL_PATH)\n",
    "\n",
    "with open(Path(TRAINED_MODEL_PATH) / \"optimal_thresholds.json\", \"r\") as f:\n",
    "    thresholds_dict = json.load(f)\n",
    "    optimal_thresholds = np.array([thresholds_dict[name] for name in CRITERIA_NAMES])\n",
    "print(\"Wczytano zoptymalizowane progi klasyfikacyjne.\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(output_dir=\"./temp_results\", per_device_eval_batch_size=4, report_to=\"none\"),\n",
    "    data_collator=default_data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# --- 2. Predykcje na zbiorze walidacyjnym ---\n",
    "print(\"Wykonywanie predykcji na zbiorze walidacyjnym...\")\n",
    "preds_output = trainer.predict(eval_dataset)\n",
    "chunk_probs = 1 / (1 + np.exp(-preds_output.predictions))\n",
    "print(\"Predykcje na poziomie fragmentów dla zbioru walidacyjnego zostały wygenerowane.\")\n",
    "\n",
    "df_eval = pd.DataFrame({\n",
    "    'doc_id': eval_dataset['doc_id'],\n",
    "    'probs': list(chunk_probs),\n",
    "    'labels': list(preds_output.label_ids)\n",
    "})\n",
    "\n",
    "doc_grouped_eval = df_eval.groupby('doc_id')\n",
    "doc_labels_map_eval = doc_grouped_eval['labels'].first().to_dict()\n",
    "\n",
    "# --- 3. Pętla ewaluacyjna na zbiorze walidacyjnym w celu znalezienia najlepszej strategii ---\n",
    "best_strategy_name = None\n",
    "best_f1_macro_score = -1.0\n",
    "\n",
    "for strategy_name, aggregation_func in AGGREGATION_STRATEGIES.items():\n",
    "    print(f\"--- Testowanie strategii na zbiorze walidacyjnym: {strategy_name} ---\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"eval_{strategy_name}\") as run:\n",
    "        mlflow.log_param(\"aggregation_strategy\", strategy_name)\n",
    "        mlflow.log_param(\"base_model_path\", TRAINED_MODEL_PATH)\n",
    "        mlflow.log_params({f\"threshold_{k}\": v for k, v in zip(CRITERIA_NAMES, optimal_thresholds)})\n",
    "\n",
    "        aggregated_probs = doc_grouped_eval['probs'].apply(\n",
    "            lambda x: aggregation_func(np.array(x.tolist()))\n",
    "        )\n",
    "        \n",
    "        doc_probs_np = np.array(aggregated_probs.tolist())\n",
    "        doc_labels_np = np.array(list(doc_labels_map_eval.values()))\n",
    "        doc_preds_np = (doc_probs_np >= optimal_thresholds).astype(int)\n",
    "        \n",
    "        final_metrics = calculate_document_metrics(doc_labels_np, doc_preds_np)\n",
    "        mlflow.log_metrics(final_metrics)\n",
    "        \n",
    "        current_f1 = final_metrics['doc_f1_macro']\n",
    "        print(f\"Wyniki dla '{strategy_name}': F1 Macro (dokument): {current_f1:.4f}\")\n",
    "\n",
    "        # Sprawdzenie i zapisanie najlepszej strategii\n",
    "        if current_f1 > best_f1_macro_score:\n",
    "            best_f1_macro_score = current_f1\n",
    "            best_strategy_name = strategy_name\n",
    "            print(f\"!!! Nowa najlepsza strategia: {best_strategy_name} z F1 Macro: {best_f1_macro_score:.4f} !!!\")\n",
    "\n",
    "print(\"\\n--- Zakończono porównywanie strategii ---\")\n",
    "print(f\"Najlepsza znaleziona strategia na zbiorze walidacyjnym: '{best_strategy_name}' (F1 Macro: {best_f1_macro_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971f0169",
   "metadata": {},
   "source": [
    "#### **Finalna ewaluacja na zbiorze testowym**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "985fd78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wykonywanie predykcji na zbiorze testowym...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Użyta strategia: percentile_75\n",
      "  F1 Macro Score (dokument): 0.6963\n",
      "  Wyniki F1-score dla poszczególnych kryteriów:\n",
      "    - c1_transition_plan: 0.8000\n",
      "    - c2_risk_management: 0.6207\n",
      "    - c4_boundaries: 0.6944\n",
      "    - c6_historical_data: 0.7595\n",
      "    - c7_intensity_metrics: 0.6364\n",
      "    - c8_targets_credibility: 0.6667\n",
      "-------------------------------------------------------\n",
      "Wyniki zapisane pomyślnie.\n",
      "🏃 View run final_test_evaluation at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160/runs/3414674617434849972182960f481d30\n",
      "🧪 View experiment at: https://dbc-26ad907d-404c.cloud.databricks.com/ml/experiments/4029214961326160\n",
      "\n",
      "Zakończono cały proces ewaluacji.\n"
     ]
    }
   ],
   "source": [
    "print(\"Wykonywanie predykcji na zbiorze testowym...\")\n",
    "test_preds_output = trainer.predict(test_dataset)\n",
    "test_chunk_probs = 1 / (1 + np.exp(-test_preds_output.predictions))\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'doc_id': test_dataset['doc_id'],\n",
    "    'probs': list(test_chunk_probs),\n",
    "    'labels': list(test_preds_output.label_ids)\n",
    "})\n",
    "\n",
    "doc_grouped_test = df_test.groupby('doc_id')\n",
    "doc_labels_map_test = doc_grouped_test['labels'].first().to_dict()\n",
    "\n",
    "# Pobranie najlepszej funkcji agregującej\n",
    "best_aggregation_func = AGGREGATION_STRATEGIES[best_strategy_name]\n",
    "\n",
    "# Agregacja prawdopodobieństw na zbiorze testowym\n",
    "aggregated_test_probs = doc_grouped_test['probs'].apply(\n",
    "    lambda x: best_aggregation_func(np.array(x.tolist()))\n",
    ")\n",
    "\n",
    "doc_probs_test_np = np.array(aggregated_test_probs.tolist())\n",
    "doc_labels_test_np = np.array(list(doc_labels_map_test.values()))\n",
    "\n",
    "# Zastosowanie progów do predykcji\n",
    "doc_preds_test_np = (doc_probs_test_np >= optimal_thresholds).astype(int)\n",
    "\n",
    "# Obliczenie metryk końcowych\n",
    "final_test_metrics = calculate_document_metrics(doc_labels_test_np, doc_preds_test_np)\n",
    "\n",
    "print(f\"Użyta strategia: {best_strategy_name}\")\n",
    "print(f\"  F1 Macro Score (dokument): {final_test_metrics['doc_f1_macro']:.4f}\")\n",
    "print(\"  Wyniki F1-score dla poszczególnych kryteriów:\")\n",
    "for name in CRITERIA_NAMES:\n",
    "    metric_key = f'doc_f1_{name}'\n",
    "    print(f\"    - {name}: {final_test_metrics.get(metric_key, 0.0):.4f}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "with mlflow.start_run(run_name=\"final_test_evaluation\") as run:\n",
    "    mlflow.log_param(\"best_aggregation_strategy\", best_strategy_name)\n",
    "    mlflow.log_param(\"base_model_path\", TRAINED_MODEL_PATH)\n",
    "    mlflow.log_params({f\"threshold_{k}\": v for k, v in zip(CRITERIA_NAMES, optimal_thresholds)})\n",
    "    \n",
    "    test_metrics_to_log = {f\"test_{k}\": v for k, v in final_test_metrics.items()}\n",
    "    mlflow.log_metrics(test_metrics_to_log)\n",
    "    print(\"Wyniki zapisane pomyślnie.\")\n",
    "\n",
    "print(\"\\nZakończono cały proces ewaluacji.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
