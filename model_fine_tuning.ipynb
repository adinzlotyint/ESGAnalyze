{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45313287",
   "metadata": {},
   "source": [
    "## **3. Fine-tuning modelu Bielik**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aec4f4",
   "metadata": {},
   "source": [
    "### **Uwaga!**\n",
    "Ze względu na poufność danych, surowe raporty i adnotacje ekspertów nie są zawarte w tym repozytorium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ceb68",
   "metadata": {},
   "source": [
    "### **Model bazowy:** speakleash/Bielik-1.5B-v3 (https://huggingface.co/speakleash/Bielik-1.5B-v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5407d1f1",
   "metadata": {},
   "source": [
    "#### **Import Bibliotek**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb6d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import plotly.express as px\n",
    "from datasets import load_from_disk, Dataset\n",
    "from sklearn.metrics import f1_score, accuracy_score, jaccard_score, hamming_loss\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    "    EvalPrediction,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb739a",
   "metadata": {},
   "source": [
    "#### **3.1. Definicja parametrów treningu oraz stałych.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "886ac528",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZED_DATA_PATH = \"data/data_tokenized\"\n",
    "MODEL_OUTPUT_PATH = \"models/Bielik-1.5B-v3-ESG\"\n",
    "MODEL_NAME = \"speakleash/Bielik-1.5B-v3\"\n",
    "\n",
    "CRITERIA_NAMES = [\n",
    "    'c1_transition_plan',\n",
    "    'c2_risk_management',\n",
    "    'c4_boundaries',\n",
    "    'c6_historical_data',\n",
    "    'c7_intensity_metrics',\n",
    "    'c8_targets_credibility',\n",
    "]\n",
    "NUM_LABELS = len(CRITERIA_NAMES)\n",
    "\n",
    "TRAINING_ARGS = {\n",
    "    \"per_device_train_batch_size\": 1,      \n",
    "    \"per_device_eval_batch_size\": 1,       \n",
    "    \"gradient_accumulation_steps\": 8,      # Zwiększa efektywny rozmiar partii do 8 (1x8), stabilizując trening.\n",
    "    \"num_train_epochs\": 3,                 # Można zmienić dla eksperymentów\n",
    "    \"learning_rate\": 2e-5,                 # Można zmienić dla eksperymentów\n",
    "    \"fp16\": True,                          # Włącza trening w trybie mieszanej precyzji, co przyspiesza i oszczędza VRAM.\n",
    "    \"logging_steps\": 50,                   \n",
    "    \"evaluation_strategy\": \"epoch\",        # Ewaluacja modelu na zbiorze walidacyjnym po każdej pełnej epoce.\n",
    "    \"save_strategy\": \"epoch\",              # Zapisuje checkpoint modelu po każdej epoce, spójnie z ewaluacją.\n",
    "    \"save_total_limit\": 2,                 # Przechowuje tylko 2 najlepsze checkpointy, oszczędzając miejsce na dysku.\n",
    "    \"load_best_model_at_end\": True,        # Po treningu automatycznie wczytuje najlepszy znaleziony model.\n",
    "    \"metric_for_best_model\": \"f1_macro\",   # Wybiera najlepszy model na podstawie F1-score\n",
    "    \"report_to\": \"none\",                   \n",
    "    \"warmup_steps\": 200,                   # Stopniowo zwiększa learning rate przez 200 kroków, stabilizując początek treningu.\n",
    "    \"weight_decay\": 0.01,                  # Dodaje lekką regularyzację (L2), aby zapobiegać przeuczeniu.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa87f425",
   "metadata": {},
   "source": [
    "#### **3.2. Wczytanie tokenizowanego zbioru.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe845134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 3475\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 563\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 683\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = load_from_disk(TOKENIZED_DATA_PATH)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5068d6f3",
   "metadata": {},
   "source": [
    "#### **3.3. Obliczanie wag klas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec99b92b",
   "metadata": {},
   "source": [
    "Ze względu na silne niezbalansowanie zbioru, obliczamy wagi dla każdej z klas. Pomoże to modelowi zwrócić większą uwagę na rzadziej występujące klasy pozytywne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34ad6059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obliczone wagi klas: ['0.70', '1.25', '0.96', '1.62', '1.13', '1.91']\n"
     ]
    }
   ],
   "source": [
    "def calculate_class_weights(dataset: Dataset) -> torch.Tensor:\n",
    "    labels = np.array(dataset['labels'])\n",
    "    pos_counts = np.sum(labels, axis=0)\n",
    "    total_samples = len(labels)\n",
    "    \n",
    "    weights = []\n",
    "    for count in pos_counts:\n",
    "        weight = total_samples / (2 * count + 1e-6) if count > 0 else 1.0\n",
    "        weights.append(weight)\n",
    "        \n",
    "    print(f\"Obliczone wagi klas: {[f'{w:.2f}' for w in weights]}\")\n",
    "    return torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "class_weights = calculate_class_weights(tokenized_datasets['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4015f4a8",
   "metadata": {},
   "source": [
    "#### **3.4. Wczytanie modelu i tokenizera.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a21d65b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at speakleash/Bielik-1.5B-v3 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd156d08",
   "metadata": {},
   "source": [
    "#### **3.5. Niestandardowa funkcja straty.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194568de",
   "metadata": {},
   "source": [
    "Focal Loss to ulepszona wersja Binary Cross-Entropy, która redukuje wagę łatwo klasyfikowanych przykładów, pozwalając modelowi skupić się na trudnych, często mniejszościowych przypadkach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faa52ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, pos_weight=None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.pos_weight = pos_weight\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, reduction='none', pos_weight=self.pos_weight\n",
    "        )\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt)**self.gamma * bce_loss\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4949cd",
   "metadata": {},
   "source": [
    "#### **3.6. Przygotowanie niestandardowej klasy Trainer z ważoną stratą.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed2911f",
   "metadata": {},
   "source": [
    "Tworzymy własną klasę Trainer, która będzie używać funkcji FocalLoss wraz z obliczonymi wagami klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62838c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESGTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights.to(self.args.device) if class_weights is not None else None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        loss_fct = FocalLoss(alpha=0.25, gamma=2.0, pos_weight=self.class_weights)\n",
    "        loss = loss_fct(logits, labels.float())\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
