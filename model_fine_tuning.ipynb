{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45313287",
   "metadata": {},
   "source": [
    "## **3. Fine-tuning modelu Bielik**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aec4f4",
   "metadata": {},
   "source": [
    "### **Uwaga!**\n",
    "Ze względu na poufność danych, surowe raporty i adnotacje ekspertów nie są zawarte w tym repozytorium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ceb68",
   "metadata": {},
   "source": [
    "### **Model bazowy:** speakleash/Bielik-1.5B-v3 (https://huggingface.co/speakleash/Bielik-1.5B-v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5407d1f1",
   "metadata": {},
   "source": [
    "#### **Import Bibliotek**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb6d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import plotly.express as px\n",
    "from datasets import load_from_disk, Dataset\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    "    EvalPrediction,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb739a",
   "metadata": {},
   "source": [
    "#### **3.1. Definicja parametrów treningu oraz stałych.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ac528",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZED_DATA_PATH = \"data/data_tokenized\"\n",
    "MODEL_OUTPUT_PATH = \"models/Bielik-1.5B-v3-ESG\"\n",
    "MODEL_NAME = \"speakleash/Bielik-1.5B-v3\"\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "CRITERIA_NAMES = [\n",
    "    'c1_transition_plan',\n",
    "    'c2_risk_management',\n",
    "    'c4_boundaries',\n",
    "    'c6_historical_data',\n",
    "    'c7_intensity_metrics',\n",
    "    'c8_targets_credibility',\n",
    "]\n",
    "NUM_LABELS = len(CRITERIA_NAMES)\n",
    "\n",
    "TRAINING_ARGS = {\n",
    "    \"per_device_train_batch_size\": 1,      \n",
    "    \"per_device_eval_batch_size\": 1,       \n",
    "    \"gradient_accumulation_steps\": 4,      \n",
    "    \"dataloader_num_workers\": 0,\n",
    "    \"num_train_epochs\": 3,                 # Można zmienić dla eksperymentów\n",
    "    \"learning_rate\": 2e-4,                 # Można zmienić dla eksperymentów\n",
    "    \"fp16\": True,                          # Włącza trening w trybie mieszanej precyzji, co przyspiesza i oszczędza VRAM.\n",
    "    \"logging_steps\": 50,                   \n",
    "    \"eval_strategy\": \"epoch\",        # Ewaluacja modelu na zbiorze walidacyjnym po każdej pełnej epoce.\n",
    "    \"save_strategy\": \"epoch\",              # Zapisuje checkpoint modelu po każdej epoce, spójnie z ewaluacją.\n",
    "    \"save_total_limit\": 2,                 # Przechowuje tylko 2 najlepsze checkpointy, oszczędzając miejsce na dysku.\n",
    "    \"load_best_model_at_end\": True,        # Po treningu automatycznie wczytuje najlepszy znaleziony model.\n",
    "    \"metric_for_best_model\": \"f1_macro\",   # Wybiera najlepszy model na podstawie F1-score\n",
    "    \"report_to\": \"none\",                   \n",
    "    \"warmup_steps\": 200,                   # Stopniowo zwiększa learning rate przez 200 kroków, stabilizując początek treningu.\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"optim\": \"paged_adamw_8bit\",\n",
    "    \"weight_decay\": 0.01,                  # Dodaje lekką regularyzację (L2), aby zapobiegać przeuczeniu.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa87f425",
   "metadata": {},
   "source": [
    "#### **3.2. Wczytanie tokenizowanego zbioru.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe845134",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = load_from_disk(TOKENIZED_DATA_PATH)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5068d6f3",
   "metadata": {},
   "source": [
    "#### **3.3. Obliczanie wag klas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec99b92b",
   "metadata": {},
   "source": [
    "Ze względu na silne niezbalansowanie zbioru, obliczamy wagi dla każdej z klas. Pomoże to modelowi zwrócić większą uwagę na rzadziej występujące klasy pozytywne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad6059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(dataset: Dataset) -> torch.Tensor:\n",
    "    labels = np.array(dataset['labels'])\n",
    "    pos_counts = np.sum(labels, axis=0)\n",
    "    total_samples = len(labels)\n",
    "    \n",
    "    weights = []\n",
    "    for count in pos_counts:\n",
    "        weight = total_samples / (2 * count + 1e-6) if count > 0 else 1.0\n",
    "        weights.append(weight)\n",
    "        \n",
    "    print(f\"Obliczone wagi klas: {[f'{w:.2f}' for w in weights]}\")\n",
    "    return torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "class_weights = calculate_class_weights(tokenized_datasets['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4015f4a8",
   "metadata": {},
   "source": [
    "#### **3.4. Wczytanie modelu i tokenizera z kwantyzacją.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21d65b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affc7786",
   "metadata": {},
   "source": [
    "#### **3.4. Definicja konfiguracji LoRA.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdeb9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd156d08",
   "metadata": {},
   "source": [
    "#### **3.5. Niestandardowa funkcja straty.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194568de",
   "metadata": {},
   "source": [
    "Focal Loss to ulepszona wersja Binary Cross-Entropy, która redukuje wagę łatwo klasyfikowanych przykładów, pozwalając modelowi skupić się na trudnych, często mniejszościowych przypadkach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa52ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, pos_weight=None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.pos_weight = pos_weight\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, reduction='none', pos_weight=self.pos_weight\n",
    "        )\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt)**self.gamma * bce_loss\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4949cd",
   "metadata": {},
   "source": [
    "#### **3.6. Przygotowanie niestandardowej klasy Trainer z ważoną stratą.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed2911f",
   "metadata": {},
   "source": [
    "Tworzymy własną klasę Trainer, która będzie używać funkcji FocalLoss wraz z obliczonymi wagami klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62838c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESGTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights.to(self.args.device) if class_weights is not None else None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        loss_fct = FocalLoss(alpha=0.25, gamma=2.0, pos_weight=self.class_weights)\n",
    "        loss = loss_fct(logits, labels.float())\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c25317a",
   "metadata": {},
   "source": [
    "#### **3.7. Przygotowanie funkcji do obliczania metryk.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad987d6",
   "metadata": {},
   "source": [
    "Definiujemy funkcję, która będzie obliczać metryki podczas ewaluacji. Używamy standardowego progu 0.5, ponieważ optymalne progi znajdziemy po zakończeniu treningu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e489102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction) -> dict:\n",
    "    logits, labels = p\n",
    "    probs = 1 / (1 + np.exp(-logits))  # Sigmoid\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "\n",
    "    f1_macro = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "    exact_match_ratio = accuracy_score(labels, preds)\n",
    "    \n",
    "    metrics = {\n",
    "        'f1_macro': f1_macro,\n",
    "        'exact_match_ratio': exact_match_ratio\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78970d5e",
   "metadata": {},
   "source": [
    "#### **3.8. Trening modelu.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35f33ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_dict = TRAINING_ARGS.copy()\n",
    "training_args_dict[\"output_dir\"] = f\"{MODEL_OUTPUT_PATH}/checkpoints\"\n",
    "training_args_dict[\"label_names\"] = [\"labels\"]\n",
    "\n",
    "training_args = TrainingArguments(**training_args_dict)\n",
    "\n",
    "trainer = ESGTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights,\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nRozpoczynam trening modelu (QLoRA)...\")\n",
    "trainer.train()\n",
    "print(\"Trening zakończony.\")\n",
    "\n",
    "output_dir = Path(MODEL_OUTPUT_PATH)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "trainer.save_model(str(output_dir)) \n",
    "tokenizer.save_pretrained(str(output_dir))\n",
    "\n",
    "print(f\"Adaptery QLoRA i tokenizer zapisane w: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
