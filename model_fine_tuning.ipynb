{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45313287",
   "metadata": {},
   "source": [
    "## **3. Fine-tuning modelu Bielik**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aec4f4",
   "metadata": {},
   "source": [
    "### **Uwaga!**\n",
    "Ze względu na poufność danych, surowe raporty i adnotacje ekspertów nie są zawarte w tym repozytorium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ceb68",
   "metadata": {},
   "source": [
    "### **Model bazowy:** speakleash/Bielik-1.5B-v3 (https://huggingface.co/speakleash/Bielik-1.5B-v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5407d1f1",
   "metadata": {},
   "source": [
    "#### **Import Bibliotek**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb6d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import plotly.express as px\n",
    "from datasets import load_from_disk, Dataset\n",
    "from sklearn.metrics import f1_score, accuracy_score, jaccard_score, hamming_loss\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    "    EvalPrediction,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb739a",
   "metadata": {},
   "source": [
    "#### **3.1. Definicja parametrów treningu oraz stałych.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "886ac528",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZED_DATA_PATH = \"data/data_tokenized\"\n",
    "MODEL_OUTPUT_PATH = \"models/Bielik-1.5B-v3-ESG\"\n",
    "MODEL_NAME = \"speakleash/Bielik-1.5B-v3\"\n",
    "\n",
    "CRITERIA_NAMES = [\n",
    "    'c1_transition_plan',\n",
    "    'c2_risk_management',\n",
    "    'c4_boundaries',\n",
    "    'c6_historical_data',\n",
    "    'c7_intensity_metrics',\n",
    "    'c8_targets_credibility',\n",
    "]\n",
    "NUM_LABELS = len(CRITERIA_NAMES)\n",
    "\n",
    "TRAINING_ARGS = {\n",
    "    \"per_device_train_batch_size\": 1,      \n",
    "    \"per_device_eval_batch_size\": 1,       \n",
    "    \"gradient_accumulation_steps\": 8,      # Zwiększa efektywny rozmiar partii do 8 (1x8), stabilizując trening.\n",
    "    \"num_train_epochs\": 3,                 # Można zmienić dla eksperymentów\n",
    "    \"learning_rate\": 2e-5,                 # Można zmienić dla eksperymentów\n",
    "    \"fp16\": True,                          # Włącza trening w trybie mieszanej precyzji, co przyspiesza i oszczędza VRAM.\n",
    "    \"logging_steps\": 50,                   \n",
    "    \"evaluation_strategy\": \"epoch\",        # Ewaluacja modelu na zbiorze walidacyjnym po każdej pełnej epoce.\n",
    "    \"save_strategy\": \"epoch\",              # Zapisuje checkpoint modelu po każdej epoce, spójnie z ewaluacją.\n",
    "    \"save_total_limit\": 2,                 # Przechowuje tylko 2 najlepsze checkpointy, oszczędzając miejsce na dysku.\n",
    "    \"load_best_model_at_end\": True,        # Po treningu automatycznie wczytuje najlepszy znaleziony model.\n",
    "    \"metric_for_best_model\": \"f1_macro\",   # Wybiera najlepszy model na podstawie F1-score\n",
    "    \"report_to\": \"none\",                   \n",
    "    \"warmup_steps\": 200,                   # Stopniowo zwiększa learning rate przez 200 kroków, stabilizując początek treningu.\n",
    "    \"weight_decay\": 0.01,                  # Dodaje lekką regularyzację (L2), aby zapobiegać przeuczeniu.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa87f425",
   "metadata": {},
   "source": [
    "#### **3.2. Wczytanie tokenizowanego zbioru.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe845134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 3475\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 563\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['labels', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 683\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = load_from_disk(TOKENIZED_DATA_PATH)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5068d6f3",
   "metadata": {},
   "source": [
    "#### **3.3. Obliczanie wag klas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec99b92b",
   "metadata": {},
   "source": [
    "Ze względu na silne niezbalansowanie zbioru, obliczamy wagi dla każdej z klas. Pomoże to modelowi zwrócić większą uwagę na rzadziej występujące klasy pozytywne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34ad6059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obliczone wagi klas: ['0.70', '1.25', '0.96', '1.62', '1.13', '1.91']\n"
     ]
    }
   ],
   "source": [
    "def calculate_class_weights(dataset: Dataset) -> torch.Tensor:\n",
    "    labels = np.array(dataset['labels'])\n",
    "    pos_counts = np.sum(labels, axis=0)\n",
    "    total_samples = len(labels)\n",
    "    \n",
    "    weights = []\n",
    "    for count in pos_counts:\n",
    "        weight = total_samples / (2 * count + 1e-6) if count > 0 else 1.0\n",
    "        weights.append(weight)\n",
    "        \n",
    "    print(f\"Obliczone wagi klas: {[f'{w:.2f}' for w in weights]}\")\n",
    "    return torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "class_weights = calculate_class_weights(tokenized_datasets['train'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
