{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac9cd781",
   "metadata": {},
   "source": [
    "## **2. Tokenizacja zbiorów danych**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbff71d",
   "metadata": {},
   "source": [
    "### **Uwaga!**\n",
    "Ze względu na poufność danych, surowe raporty i adnotacje ekspertów nie są zawarte w tym repozytorium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa9146",
   "metadata": {},
   "source": [
    "### **Problem:** \n",
    "Wieloetykietowa klasyfikacja długich, nieustrukturyzowanych dokumentów w języku polskim, w warunkach silnego niezbalansowania klas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba59e66",
   "metadata": {},
   "source": [
    "### **Model enkodera:** sdadas/polish-longformer-base-4096 (https://huggingface.co/sdadas/polish-longformer-base-4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb90f0b4",
   "metadata": {},
   "source": [
    "#### **Import Bibliotek**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aea2793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import LongformerTokenizerFast\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53524a16",
   "metadata": {},
   "source": [
    "Ustawienie sztywnych parametrów dla okna kontekstowego zgodnie z dokumentacją modelu Bielik-1.5B-v3 oraz narzucenie wymiaru okna przesuwnego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fed7bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"sdadas/polish-longformer-base-4096\"\n",
    "MAX_LENGTH = 4096\n",
    "STRIDE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4221adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_json('data/data_split/train.jsonl', lines=True)\n",
    "df_val = pd.read_json('data/data_split/validation.jsonl', lines=True)\n",
    "df_test = pd.read_json('data/data_split/test.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8a0f49",
   "metadata": {},
   "source": [
    "#### **Przygotowanie do tokenizacji.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "611dca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "val_dataset = Dataset.from_pandas(df_val)\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3185328",
   "metadata": {},
   "source": [
    "Wczytanie odpowiedniego tokenizera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b80fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizerFast.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108160c9",
   "metadata": {},
   "source": [
    "#### **Przeprowadzenie tokenizacji.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7dc5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_and_tokenize(examples):\n",
    "    tokenized_output = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        stride=STRIDE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=False,\n",
    "    )\n",
    "\n",
    "    # Pobieramy mapowanie z nowych, podzielonych fragmentów do oryginalnych przykładów\n",
    "    sample_mapping = tokenized_output.pop(\"overflow_to_sample_mapping\")\n",
    "    \n",
    "    # Tworzymy listę etykiet dla każdego nowego fragmentu i dodajemy numer id\n",
    "    labels = []\n",
    "    doc_ids = []\n",
    "    for i in range(len(tokenized_output[\"input_ids\"])):\n",
    "        original_sample_idx = sample_mapping[i]\n",
    "        labels.append(examples[\"labels\"][original_sample_idx])\n",
    "        doc_ids.append(examples[\"doc_id\"][original_sample_idx])\n",
    "    \n",
    "    tokenized_output[\"labels\"] = labels\n",
    "    tokenized_output[\"doc_id\"] = doc_ids\n",
    "    return tokenized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa8a8e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0b19eb39fa42fc8e7d0e9e67c7555d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/275 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2803f16503624b78a4fe3c38b06207a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/59 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd76f6d76cc498993f6699887cd3bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/59 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    chunk_and_tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    "    num_proc=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dcceb1",
   "metadata": {},
   "source": [
    "#### **Zapis wyników tokenizacji.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ac0cb52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde08251cda14c3bbb5f9aa68dad25b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5070 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ad6273a83b46e89c8ddcf384149830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1062 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24216cdaadfc45beb46ad39ea83f6e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/929 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "OUTPUT_DIR = \"data/data_tokenized\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "tokenized_datasets.save_to_disk(OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
