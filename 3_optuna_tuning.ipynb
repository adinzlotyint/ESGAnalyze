{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edc3bdac",
   "metadata": {},
   "source": [
    "## **3. Dobór hiperparametrów**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f578eb47",
   "metadata": {},
   "source": [
    "#### **Import Bibliotek**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d8f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import optuna\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "from datasets import load_from_disk, Dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import (\n",
    "    LongformerForSequenceClassification,\n",
    "    LongformerConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EvalPrediction,\n",
    "    EarlyStoppingCallback,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e87d3b",
   "metadata": {},
   "source": [
    "#### **Konfiguracja Stałych**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e122da",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZED_DATA_PATH = \"data/data_tokenized\"\n",
    "MODEL_NAME = \"sdadas/polish-longformer-base-4096\"\n",
    "NUM_LABELS = 6\n",
    "\n",
    "TRAIN_SUBSET_FRACTION = 0.3  # Używamy 30% danych do szybkiego tuningu\n",
    "TUNE_SPLIT_TEST_SIZE = 0.2   # Podział podzbioru na 80% treningu / 20% walidacji\n",
    "MAX_EPOCHS_PER_TRIAL = 8     # Maksymalna długość pojedynczej próby\n",
    "N_TRIALS  = 20     # Docelowa liczba prób do przeprowadzenia\n",
    "\n",
    "# Parametry ustawione na podstawie wiedzy domenowej i standardowej implementacji \n",
    "FOCAL_ALPHA = 0.5\n",
    "FOCAL_GAMMA = 2.0\n",
    "\n",
    "OPTUNA_STORAGE_DB = \"sqlite:///optuna_study.db\"\n",
    "OPTUNA_STUDY_NAME = \"esg-longformer-study-v2\"\n",
    "TRIALS_CHECKPOINT_DIR = \"optuna_model_checkpoints\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5eed35",
   "metadata": {},
   "source": [
    "#### **Focal Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abe85a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma, pos_weight=None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.pos_weight = pos_weight\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, reduction='none', pos_weight=self.pos_weight\n",
    "        )\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt)**self.gamma * bce_loss\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8fec5d",
   "metadata": {},
   "source": [
    "#### **Klasa ESGTrainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b962f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESGTrainer(Trainer):\n",
    "    def __init__(self, *args, focal_loss_alpha=0.5, focal_loss_gamma=2.0, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights.to(self.args.device) if class_weights is not None else None\n",
    "        self.loss_fct = FocalLoss(alpha=focal_loss_alpha, gamma=focal_loss_gamma, pos_weight=self.class_weights)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss = self.loss_fct(logits, labels.float())\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054ccf4",
   "metadata": {},
   "source": [
    "#### **Obliczanie wag**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2ad0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(dataset: Dataset) -> torch.Tensor:\n",
    "    labels = np.array(dataset['labels'])\n",
    "    pos_counts = np.sum(labels, axis=0)\n",
    "    total_samples = len(labels)\n",
    "    weights = [total_samples / (2 * count + 1e-6) if count > 0 else 1.0 for count in pos_counts]\n",
    "    return torch.tensor(weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb1623a",
   "metadata": {},
   "source": [
    "#### **Inicjalizacja modelu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab0a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(trial):\n",
    "    model_config = LongformerConfig.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS, problem_type=\"multi_label_classification\")\n",
    "    return LongformerForSequenceClassification.from_pretrained(MODEL_NAME, config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680ac8a",
   "metadata": {},
   "source": [
    "#### **Krok 1: Przygotowanie danych do optymalizacji**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = load_from_disk(TOKENIZED_DATA_PATH)\n",
    "\n",
    "subset_size = int(len(full_dataset[\"train\"]) * TRAIN_SUBSET_FRACTION)\n",
    "data_subset = full_dataset[\"train\"].shuffle(seed=42).select(range(subset_size))\n",
    "split_data = data_subset.train_test_split(test_size=TUNE_SPLIT_TEST_SIZE, seed=42)\n",
    "\n",
    "tune_train_dataset = split_data[\"train\"]\n",
    "tune_val_dataset = split_data[\"test\"]\n",
    "class_weights = calculate_class_weights(full_dataset['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d690f054",
   "metadata": {},
   "source": [
    "#### **Krok 2: Uruchomienie optymalizacji**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792b9138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_space_optuna(trial: optuna.Trial) -> Dict[str, float]:\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.01, 0.2, log=True),\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./optuna_temp\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=MAX_EPOCHS_PER_TRIAL,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = ESGTrainer(\n",
    "    args=training_args,\n",
    "    model_init=model_init,\n",
    "    train_dataset=tune_train_dataset,\n",
    "    eval_dataset=tune_val_dataset,\n",
    "    compute_metrics=lambda p: {'eval_f1_macro': f1_score(p.label_ids, (1 / (1 + np.exp(-p.predictions)) > 0.5).astype(int), average='macro', zero_division=0)},\n",
    "    class_weights=class_weights,\n",
    "    focal_loss_alpha=FOCAL_ALPHA,\n",
    "    focal_loss_gamma=FOCAL_GAMMA,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "best_run = trainer.hyperparameter_search(\n",
    "    hp_space=hp_space_optuna,\n",
    "    backend=\"optuna\",\n",
    "    n_trials=N_TRIALS,\n",
    "    direction=\"maximize\",\n",
    "    compute_objective=lambda metrics: metrics[\"eval_f1_macro\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d444d1b5",
   "metadata": {},
   "source": [
    "#### **Najlepsza zarejestrowana próba:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a164f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[1216/1216 2:25:23, Epoch 8/8]\n",
    "Step\tTraining Loss\t  Validation Loss\t  F1 Macro\n",
    "100\t  No log\t        0.102660\t        0.592207\n",
    "200\t  No log\t        0.083195\t        0.662340\n",
    "300\t  No log\t        0.065447\t        0.755528\n",
    "400\t  No log\t        0.052853\t        0.806245\n",
    "500\t  0.076500\t      0.047519\t        0.834683\n",
    "600\t  0.076500\t      0.040097\t        0.861286\n",
    "700\t  0.076500\t      0.034386\t        0.884033\n",
    "800\t  0.076500\t      0.033461\t        0.890963\n",
    "900\t  0.076500\t      0.032104\t        0.894759\n",
    "1000\t0.025600\t      0.029985\t        0.901850\n",
    "1100\t0.025600\t      0.028734\t        0.906202\n",
    "1200\t0.025600\t      0.028170\t        0.904163\n",
    "\n",
    "[I 2025-08-26 23:57:19,542] Trial 0 finished with value: 0.9041625781021257 and parameters: {'learning_rate': 2.2106899831932417e-05, 'weight_decay': 0.15910997111952396}.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
